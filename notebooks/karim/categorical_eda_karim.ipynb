{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Categorical EDA (Karim) - простой рабочий ноутбук\n\nЗапускай ячейки сверху вниз. Если ловишь редкую ошибку `pandas.period already defined`,\nперезапусти Kernel и снова Run All.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nfrom pyarrow.lib import ArrowKeyError\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import chi2_contingency, kruskal\nfrom sklearn.feature_selection import mutual_info_classif\n\nsns.set_theme(style='whitegrid')\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== Прямые пути (без авто-магии) ======\nPROJECT_ROOT = Path('/Users/karimkhabib/Documents/Projects Programming/PyCharm/data-fusion-contest')\nDATA_DIR = PROJECT_ROOT / 'src' / 'data'\nOUT_DIR = PROJECT_ROOT / 'outputs' / 'categorical_analysis'\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nLABELS_PATH = DATA_DIR / 'train_labels.parquet'\nTRAIN_FILES = [\n    DATA_DIR / 'train_part_1.parquet',\n    DATA_DIR / 'train_part_2.parquet',\n    DATA_DIR / 'train_part_3.parquet',\n]\n\n# Быстрый режим для чернового EDA\nQUICK_MODE = False\nMAX_LABEL_EVENTS = 200_000\nBATCH_SIZE = 500_000\nMISSING_TOKEN = '__MISSING__'\n\nprint('LABELS_PATH:', LABELS_PATH)\nprint('TRAIN FILES:')\nfor x in TRAIN_FILES:\n    print(' -', x)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def safe_read_parquet(path, columns=None):\n    # Чтение parquet с fallback от ArrowKeyError в ноутбуке\n    try:\n        return pd.read_parquet(path, columns=columns)\n    except ArrowKeyError:\n        import pyarrow as pa\n        for ext_name in ('pandas.period', 'pandas.interval'):\n            try:\n                pa.unregister_extension_type(ext_name)\n            except Exception:\n                pass\n        table = pq.read_table(path, columns=columns)\n        return table.to_pandas()\n\nlabels = safe_read_parquet(LABELS_PATH)\nlabels = labels[['event_id', 'target']].copy()\nlabels['event_id'] = pd.to_numeric(labels['event_id'], errors='coerce').astype('Int64')\nlabels['target'] = pd.to_numeric(labels['target'], errors='coerce').astype('Int64')\nlabels = labels.dropna(subset=['event_id', 'target']).copy()\nlabels['event_id'] = labels['event_id'].astype('int64')\nlabels['target'] = labels['target'].astype('int8')\nlabels = labels.drop_duplicates('event_id', keep='last').reset_index(drop=True)\n\nprint('labels shape:', labels.shape)\nprint(labels['target'].value_counts())\nprint('positive rate:', round(float(labels['target'].mean()), 6))\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CATEGORICAL_COLUMNS = [\n    'event_type_nm',\n    'event_desc',\n    'channel_indicator_type',\n    'channel_indicator_sub_type',\n    'currency_iso_cd',\n    'mcc_code',\n    'pos_cd',\n    'accept_language',\n    'browser_language',\n    'timezone',\n    'operating_system_type',\n    'device_system_version',\n    'screen_size',\n    'developer_tools',\n    'phone_voip_call_state',\n    'web_rdp_connection',\n    'compromised',\n]\n\nsample_cols = safe_read_parquet(TRAIN_FILES[0]).columns.tolist()\nAVAILABLE_CAT_COLS = [c for c in CATEGORICAL_COLUMNS if c in sample_cols]\nprint('available categorical columns:', len(AVAILABLE_CAT_COLS))\nprint(AVAILABLE_CAT_COLS)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "label_event_ids = labels['event_id'].tolist()\nif QUICK_MODE and len(label_event_ids) > MAX_LABEL_EVENTS:\n    rng = np.random.default_rng(42)\n    label_event_ids = rng.choice(label_event_ids, size=MAX_LABEL_EVENTS, replace=False).tolist()\nlabel_event_ids = set(label_event_ids)\n\nprint('using label event ids:', len(label_event_ids))\n\n\ndef collect_labeled_rows(train_files, label_event_ids, feature_columns, batch_size=500_000):\n    chunks = []\n    read_cols = ['event_id'] + feature_columns\n\n    for file_path in train_files:\n        pf = pq.ParquetFile(file_path)\n        for batch in pf.iter_batches(columns=read_cols, batch_size=batch_size):\n            part = batch.to_pandas()\n            mask = part['event_id'].isin(label_event_ids)\n            if mask.any():\n                chunks.append(part.loc[mask, read_cols].copy())\n\n    if not chunks:\n        return pd.DataFrame(columns=read_cols)\n\n    out = pd.concat(chunks, ignore_index=True)\n    out = out.drop_duplicates('event_id', keep='last')\n    return out\n\nlabeled_features = collect_labeled_rows(\n    TRAIN_FILES,\n    label_event_ids,\n    AVAILABLE_CAT_COLS,\n    batch_size=BATCH_SIZE,\n)\n\nprint('labeled feature rows:', labeled_features.shape)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df = labels.merge(labeled_features, on='event_id', how='inner')\nprint('merged shape:', df.shape)\nprint('merged positive rate:', round(float(df['target'].mean()), 6))\ndf.head(3)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prep_cat(s):\n    return s.astype('string').fillna(MISSING_TOKEN)\n\n\ndef calc_cramers_v(cat, y):\n    table = pd.crosstab(cat, y)\n    if table.shape[0] < 2 or table.shape[1] < 2:\n        return np.nan, np.nan, np.nan\n\n    chi2, p, _, _ = chi2_contingency(table)\n    n = table.to_numpy().sum()\n    if n <= 1:\n        return chi2, p, np.nan\n\n    phi2 = chi2 / n\n    r, k = table.shape\n    phi2_corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n    r_corr = r - ((r - 1) ** 2) / (n - 1)\n    k_corr = k - ((k - 1) ** 2) / (n - 1)\n    denom = min(k_corr - 1, r_corr - 1)\n    v = np.sqrt(phi2_corr / denom) if denom > 0 else np.nan\n    return chi2, p, v\n\n\ndef calc_mi(cat, y):\n    codes, _ = pd.factorize(cat, sort=False)\n    return mutual_info_classif(codes.reshape(-1, 1), y.values, discrete_features=True, random_state=42)[0]\n\n\ndef calc_kruskal(cat, y):\n    groups = [y[cat == val].values for val in cat.unique()]\n    groups = [g for g in groups if len(g) >= 2]\n    if len(groups) < 2:\n        return np.nan, np.nan\n    h, p = kruskal(*groups, nan_policy='omit')\n    return h, p\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "metrics_rows = []\ny = df['target'].astype('int8')\n\nfor col in AVAILABLE_CAT_COLS:\n    cat = prep_cat(df[col])\n    chi2_stat, chi2_p, c_v = calc_cramers_v(cat, y)\n    mi = calc_mi(cat, y)\n    kr_h, kr_p = calc_kruskal(cat, y)\n\n    metrics_rows.append({\n        'feature': col,\n        'n_unique': int(cat.nunique(dropna=False)),\n        'missing_rate': float((cat == MISSING_TOKEN).mean()),\n        'mutual_info': float(mi),\n        'cramers_v': float(c_v) if pd.notna(c_v) else np.nan,\n        'chi2_p_value': float(chi2_p) if pd.notna(chi2_p) else np.nan,\n        'kruskal_p_value': float(kr_p) if pd.notna(kr_p) else np.nan,\n    })\n\nmetrics = pd.DataFrame(metrics_rows).sort_values(['mutual_info', 'cramers_v'], ascending=False)\nmetrics\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plt.figure(figsize=(10, 6))\nsns.barplot(data=metrics.head(12), y='feature', x='mutual_info', color='#4C78A8')\nplt.title('Top categorical features by mutual_info')\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=metrics.sort_values('cramers_v', ascending=False).head(12), y='feature', x='cramers_v', color='#F58518')\nplt.title(\"Top categorical features by Cramer's V\")\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def summarize_categorical(df, feature, min_count=30):\n    cat = prep_cat(df[feature])\n    tmp = pd.DataFrame({'cat': cat, 'target': df['target'].astype('int8')})\n    summary = tmp.groupby('cat', dropna=False)['target'].agg(['count', 'mean', 'sum']).rename(\n        columns={'mean': 'target_rate', 'sum': 'target_positives'}\n    )\n    summary = summary[summary['count'] >= min_count].sort_values('target_rate', ascending=False)\n    return summary\n\nTOP_FEATURES = metrics['feature'].head(5).tolist()\nTOP_FEATURES\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "for feat in TOP_FEATURES:\n    print('\n' + '=' * 90)\n    print('Feature:', feat)\n    summary = summarize_categorical(df, feat, min_count=30).head(20)\n    display(summary)\n\n    if not summary.empty:\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x=summary['target_rate'].values, y=summary.index.astype(str), color='#54A24B')\n        plt.title(f'{feat}: top categories by target_rate')\n        plt.tight_layout()\n        plt.show()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "metrics.to_csv(OUT_DIR / 'categorical_feature_metrics.csv', index=False)\n\nfeature_summary_dir = OUT_DIR / 'feature_summaries'\nfeature_summary_dir.mkdir(parents=True, exist_ok=True)\n\nfor feat in AVAILABLE_CAT_COLS:\n    s = summarize_categorical(df, feat, min_count=1)\n    s.to_csv(feature_summary_dir / f'{feat}_summary.csv')\n\nprint('saved:', OUT_DIR / 'categorical_feature_metrics.csv')\nprint('saved dir:', feature_summary_dir)\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}