{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Categorical EDA (Karim)\n\nЭтот ноутбук сделан под задачу Data Fusion Contest.\n\nЧто делает:\n- быстро загружает `train_labels` из `.parquet` или `.csv`;\n- батчами читает `train_part_*.parquet` и берет только размеченные `event_id`;\n- считает влияние категориальных фич на `target`;\n- строит графики и сохраняет CSV-результаты.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import chi2_contingency, kruskal\nfrom sklearn.feature_selection import mutual_info_classif\n\nsns.set_theme(style='whitegrid')\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Автоопределение корня проекта\nif (Path.cwd() / 'src' / 'data').exists():\n    REPO_ROOT = Path.cwd()\nelif (Path.cwd().parent / 'src' / 'data').exists():\n    REPO_ROOT = Path.cwd().parent\nelse:\n    raise FileNotFoundError('Не нашел папку src/data. Открой ноутбук из корня репозитория или notebooks/.')\n\nDATA_DIR = REPO_ROOT / 'src' / 'data'\nOUT_DIR = REPO_ROOT / 'outputs' / 'categorical_analysis'\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint('REPO_ROOT =', REPO_ROOT)\nprint('DATA_DIR =', DATA_DIR)\nprint('OUT_DIR  =', OUT_DIR)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Поиск train labels (parquet/csv)\nlabel_candidates = sorted(DATA_DIR.glob('*label*.parquet')) + sorted(DATA_DIR.glob('*label*.csv'))     + sorted(DATA_DIR.glob('*target*.parquet')) + sorted(DATA_DIR.glob('*target*.csv'))\n\nif not label_candidates:\n    raise FileNotFoundError(\n        'Не найден labels файл. Положи его в src/data, например train_labels.parquet или train_labels.csv'\n    )\n\nLABELS_PATH = label_candidates[0]\nTRAIN_FILES = sorted(DATA_DIR.glob('train_part_*.parquet'))\n\nif not TRAIN_FILES:\n    raise FileNotFoundError('Не найдены train_part_*.parquet в src/data')\n\nprint('Labels file:', LABELS_PATH.name)\nprint('Train parts:')\nfor f in TRAIN_FILES:\n    print(' -', f.name)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_labels(labels_path: Path) -> pd.DataFrame:\n    if labels_path.suffix.lower() == '.parquet':\n        labels = pd.read_parquet(labels_path)\n    else:\n        labels = pd.read_csv(labels_path)\n\n    cols_lower = {c.lower(): c for c in labels.columns}\n\n    event_col = None\n    for c in ['event_id', 'eventid', 'event-id']:\n        if c in cols_lower:\n            event_col = cols_lower[c]\n            break\n\n    target_col = None\n    for c in ['target', 'label', 'y', 'class']:\n        if c in cols_lower:\n            target_col = cols_lower[c]\n            break\n\n    if event_col is None or target_col is None:\n        raise ValueError(\n            f'Не нашел event_id/target в labels. Колонки: {list(labels.columns)}'\n        )\n\n    out = labels[[event_col, target_col]].rename(columns={event_col: 'event_id', target_col: 'target'}).copy()\n    out['event_id'] = pd.to_numeric(out['event_id'], errors='coerce')\n    out['target'] = pd.to_numeric(out['target'], errors='coerce')\n    out = out.dropna(subset=['event_id', 'target'])\n\n    out['event_id'] = out['event_id'].astype('int64')\n    out['target'] = out['target'].astype('int8')\n    out = out.drop_duplicates('event_id', keep='last').reset_index(drop=True)\n    return out\n\nlabels = load_labels(LABELS_PATH)\nprint('Labels shape:', labels.shape)\nprint('Target distribution:')\nprint(labels['target'].value_counts(dropna=False))\nprint('Positive rate:', labels['target'].mean().round(6))\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Категориальные колонки из описания + проверка наличия\nCATEGORICAL_COLUMNS = [\n    'event_type_nm',\n    'event_desc',\n    'channel_indicator_type',\n    'channel_indicator_sub_type',\n    'currency_iso_cd',\n    'mcc_code',\n    'pos_cd',\n    'accept_language',\n    'browser_language',\n    'timezone',\n    'operating_system_type',\n    'device_system_version',\n    'screen_size',\n    'developer_tools',\n    'phone_voip_call_state',\n    'web_rdp_connection',\n    'compromised',\n]\n\nsample_cols = pd.read_parquet(TRAIN_FILES[0], columns=None).columns.tolist()\navailable_cols = [c for c in CATEGORICAL_COLUMNS if c in sample_cols]\nmissing_cols = [c for c in CATEGORICAL_COLUMNS if c not in sample_cols]\n\nprint('Available categorical columns:', len(available_cols))\nprint(available_cols)\nif missing_cols:\n    print('Missing columns:', missing_cols)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Режимы загрузки\nQUICK_MODE = False          # True -> быстрее для первичного осмотра\nMAX_LABEL_EVENTS = 200_000  # используется только если QUICK_MODE=True\nBATCH_SIZE = 500_000\nMISSING_TOKEN = '__MISSING__'\n\nlabel_event_ids = labels['event_id'].tolist()\nif QUICK_MODE and len(label_event_ids) > MAX_LABEL_EVENTS:\n    rng = np.random.default_rng(42)\n    label_event_ids = rng.choice(label_event_ids, size=MAX_LABEL_EVENTS, replace=False).tolist()\n\nlabel_event_ids = set(label_event_ids)\nprint('Using label events:', len(label_event_ids))\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def collect_labeled_rows(train_files, label_event_ids, feature_columns, batch_size=500_000):\n    out = []\n    read_cols = ['event_id'] + feature_columns\n\n    for file_path in train_files:\n        pf = pq.ParquetFile(file_path)\n        for batch in pf.iter_batches(columns=read_cols, batch_size=batch_size):\n            chunk = batch.to_pandas()\n            mask = chunk['event_id'].isin(label_event_ids)\n            if mask.any():\n                out.append(chunk.loc[mask, read_cols].copy())\n\n    if not out:\n        return pd.DataFrame(columns=read_cols)\n\n    res = pd.concat(out, ignore_index=True)\n    res = res.drop_duplicates('event_id', keep='last')\n    return res\n\nlabeled_features = collect_labeled_rows(\n    TRAIN_FILES,\n    label_event_ids=label_event_ids,\n    feature_columns=available_cols,\n    batch_size=BATCH_SIZE,\n)\n\nprint('Collected labeled feature rows:', labeled_features.shape)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df = labels.merge(labeled_features, on='event_id', how='inner')\nprint('Merged shape:', df.shape)\nprint('Target rate after merge:', df['target'].mean().round(6))\n\nif df.empty:\n    raise RuntimeError('После merge данных 0 строк. Проверь соответствие event_id в train_part и labels.')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _prep_cat(s: pd.Series) -> pd.Series:\n    return s.astype('string').fillna(MISSING_TOKEN)\n\n\ndef cramers_v(cat: pd.Series, y: pd.Series):\n    ctab = pd.crosstab(cat, y)\n    if ctab.shape[0] < 2 or ctab.shape[1] < 2:\n        return np.nan, np.nan, np.nan\n\n    chi2, p, _, _ = chi2_contingency(ctab)\n    n = ctab.to_numpy().sum()\n    if n <= 1:\n        return chi2, p, np.nan\n\n    phi2 = chi2 / n\n    r, k = ctab.shape\n    phi2_corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n    r_corr = r - ((r - 1) ** 2) / (n - 1)\n    k_corr = k - ((k - 1) ** 2) / (n - 1)\n    denom = min(k_corr - 1, r_corr - 1)\n    v = np.sqrt(phi2_corr / denom) if denom > 0 else np.nan\n    return chi2, p, v\n\n\ndef cat_mutual_info(cat: pd.Series, y: pd.Series):\n    codes, _ = pd.factorize(cat, sort=False)\n    return mutual_info_classif(codes.reshape(-1, 1), y.values, discrete_features=True, random_state=42)[0]\n\n\ndef kruskal_for_cat(cat: pd.Series, y: pd.Series):\n    groups = [y[cat == c].values for c in cat.unique()]\n    groups = [g for g in groups if len(g) >= 2]\n    if len(groups) < 2:\n        return np.nan, np.nan\n    h, p = kruskal(*groups, nan_policy='omit')\n    return h, p\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "rows = []\nfor col in available_cols:\n    cat = _prep_cat(df[col])\n    y = df['target'].astype('int8')\n\n    chi2_stat, chi2_p, c_v = cramers_v(cat, y)\n    mi = cat_mutual_info(cat, y)\n    kr_h, kr_p = kruskal_for_cat(cat, y)\n\n    rows.append({\n        'feature': col,\n        'n_unique': int(cat.nunique(dropna=False)),\n        'missing_rate': float((cat == MISSING_TOKEN).mean()),\n        'mutual_info': float(mi),\n        'chi2_stat': float(chi2_stat) if pd.notna(chi2_stat) else np.nan,\n        'chi2_p_value': float(chi2_p) if pd.notna(chi2_p) else np.nan,\n        'cramers_v': float(c_v) if pd.notna(c_v) else np.nan,\n        'kruskal_h': float(kr_h) if pd.notna(kr_h) else np.nan,\n        'kruskal_p_value': float(kr_p) if pd.notna(kr_p) else np.nan,\n    })\n\nmetrics = pd.DataFrame(rows).sort_values(['mutual_info', 'cramers_v'], ascending=False)\nmetrics\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Визуализация топ фич по MI\nplt.figure(figsize=(10, 6))\nplot_df = metrics.head(12).copy()\nsns.barplot(data=plot_df, y='feature', x='mutual_info', color='#4C78A8')\nplt.title('Top categorical features by mutual information')\nplt.xlabel('Mutual information')\nplt.ylabel('Feature')\nplt.tight_layout()\nplt.show()\n\n# Визуализация топ фич по Cramer's V\nplt.figure(figsize=(10, 6))\nplot_df = metrics.sort_values('cramers_v', ascending=False).head(12).copy()\nsns.barplot(data=plot_df, y='feature', x='cramers_v', color='#F58518')\nplt.title(\"Top categorical features by Cramer's V\")\nplt.xlabel(\"Cramer's V\")\nplt.ylabel('Feature')\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def summarize_categorical(df, feature, min_count=50, top_n=20):\n    cat = _prep_cat(df[feature])\n    tmp = pd.DataFrame({'cat': cat, 'target': df['target'].astype('int8')})\n    summary = tmp.groupby('cat', dropna=False)['target'].agg(['count', 'mean', 'sum']).rename(\n        columns={'mean': 'target_rate', 'sum': 'target_positives'}\n    )\n    summary['target_negatives'] = summary['count'] - summary['target_positives']\n    summary = summary[summary['count'] >= min_count].sort_values('target_rate', ascending=False)\n    return summary.head(top_n)\n\n\ndef plot_feature_profile(df, feature, min_count=50, top_n=20):\n    summary = summarize_categorical(df, feature, min_count=min_count, top_n=top_n)\n    display(summary)\n\n    if summary.empty:\n        print(f'No categories with count >= {min_count} for {feature}')\n        return\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=summary['target_rate'].values, y=summary.index.astype(str), color='#54A24B')\n    plt.title(f'{feature}: target_rate for top categories (count >= {min_count})')\n    plt.xlabel('Target rate')\n    plt.ylabel('Category')\n    plt.tight_layout()\n    plt.show()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Посмотри профили для самых сильных фич\nTOP_FEATURES_TO_INSPECT = metrics['feature'].head(5).tolist()\nTOP_FEATURES_TO_INSPECT\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "for feat in TOP_FEATURES_TO_INSPECT:\n    print('\n' + '=' * 80)\n    print('Feature:', feat)\n    plot_feature_profile(df, feat, min_count=30, top_n=20)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Сохранение результатов\nmetrics.to_csv(OUT_DIR / 'categorical_feature_metrics.csv', index=False)\n\nfeature_summary_dir = OUT_DIR / 'feature_summaries'\nfeature_summary_dir.mkdir(parents=True, exist_ok=True)\nfor feat in available_cols:\n    summary = summarize_categorical(df, feat, min_count=1, top_n=1000000)\n    summary.to_csv(feature_summary_dir / f'{feat}_summary.csv')\n\nprint('Saved:')\nprint(' -', OUT_DIR / 'categorical_feature_metrics.csv')\nprint(' -', feature_summary_dir)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Как интерпретировать\n\n- `mutual_info` и `cramers_v` выше -> признак обычно полезнее.\n- Очень высокий `missing_rate` не значит, что фича плохая: иногда сам факт пропуска информативен.\n- Смотри не только на статистику, но и на стабильность: категории с очень маленьким `count` часто шумные.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}